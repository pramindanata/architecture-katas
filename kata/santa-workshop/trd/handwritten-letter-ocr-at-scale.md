# Handwritten Letter OCR at Scale

> Details of this doc is generated by Claude AI to support the ADR04.

## Problem Statement
- Extract text from handwritten letter images and parse into JSON structure
- **Traffic pattern**: Seasonal burst to 100K req/s during 2-week Christmas period, minimal traffic rest of year
- **Constraint**: 95% unique inputs (no caching benefit)

---

## **RECOMMENDED APPROACH: Cloud-Based Burst Architecture**

### Architecture Overview
```
API Gateway → SQS Queue → Kubernetes (GPU Auto-scaling) → Results Storage
                          ├─ OCR: TrOCR (2000 pods peak)
                          └─ Parser: Llama 3.1 8B (1000 pods peak)
```

### Key Components
1. **Queue-based async processing** (60-second SLA, not real-time)
2. **Self-hosted models**: TrOCR for OCR + Llama 3.1 8B for parsing
3. **Elastic cloud infrastructure**: AWS EKS or GCP GKE with GPU autoscaling
4. **Spot instances**: 70% of fleet on spot for cost savings
5. **Batch inference**: Process 16-32 images per GPU call (5-10x throughput boost)
6. **Model optimization**: INT8 quantization (2x faster, minimal accuracy loss)
7. **Multi-cloud failover**: Primary AWS, backup GCP (Christmas GPU capacity is competitive)

### Cost Structure
- **Normal period (50 weeks)**: $5K-15K/month
- **Christmas period (2 weeks)**: $500K-1M
- **Annual total**: **$1M-1.5M**

### Why This Works
✅ **Pay-per-use**: Only pay for massive infrastructure during 2-week peak  
✅ **Elastic scaling**: Cloud handles 10-100x traffic spikes automatically  
✅ **No capex**: Zero upfront hardware investment  
✅ **Proven technology**: TrOCR and Llama are production-ready, well-documented  
✅ **Cost-effective**: 10-50x cheaper than API solutions at this scale  
✅ **Realistic timeline**: 3-4 months to production with 3-4 ML engineers  

### Critical Success Factors
- **Pre-warming**: Scale up gradually starting 2 weeks before Christmas
- **GPU reservation**: Reserve cloud GPU capacity early (Nov/Dec are competitive)
- **Load testing**: Full-scale test in November at 50% peak traffic
- **Fine-tuning**: Train models on actual handwritten samples for accuracy

---

## Other Approaches Considered (NOT Recommended)

### ❌ **Pure API Approach** (OpenAI, Claude, Gemini)

**Why not:**
- **Prohibitive cost**: $50M-100M annually for Christmas traffic alone
- **Rate limits**: Even enterprise tiers max out at hundreds of req/s, not 100K
- **No distribution strategy**: Even splitting across all major providers doesn't reach required scale
- **Cost breakdown**: $0.0065-0.0115 per request × 8.64B Christmas requests = $56M-99M just for 2 weeks

**Verdict**: 50-100x more expensive than self-hosted, physically impossible to get enough API quota.

---

### ❌ **Hybrid OCR API + LLM Approach**

**Architecture**: Google Vision/Azure OCR → Claude/GPT for parsing

**Why not:**
- **Still too expensive**: $1.7M-5.6M per month during peak
- **Rate limit bottleneck**: LLM APIs still cap at low thousands req/s
- **Complexity**: Managing two separate API services with different rate limits
- **No cost benefit**: Only marginally cheaper than pure vision LLM approach

**Verdict**: Doesn't solve the fundamental API rate limit problem, still 3-5x more expensive than self-hosted.

---

### ⚠️ **Hybrid (On-Premise Base + Cloud Burst)**

**Architecture**: Own 50-100 GPUs for baseline + rent cloud GPUs for Christmas

**Why not recommended (but viable):**
- **High capex**: $500K-1M upfront hardware investment
- **Better only long-term**: Takes 2-3 years to achieve better ROI than pure cloud
- **Maintenance overhead**: Requires dedicated ops team year-round for underutilized hardware
- **Limited benefit**: Baseline traffic is too low to justify owned hardware
- **Annual cost**: $2M-3M (year 1), $1.5M-2M (year 3+)

**Verdict**: Makes sense only if you have multi-year commitment and existing GPU infrastructure. For a new system with seasonal traffic, pure cloud is superior.

---

### ❌ **Year-Round Large Infrastructure**

**Architecture**: Maintain 1,500-2,000 GPUs continuously

**Why not:**
- **Massive waste**: Infrastructure sits 98% idle for 50 weeks
- **Cost**: $1.5M-6M per month year-round = $18M-72M annually
- **Complexity**: Managing thousands of GPUs requires large DevOps team
- **Capex**: $15M-45M for on-premise option

**Verdict**: Economically irrational for seasonal workload. Only makes sense if traffic is sustained year-round.

---

### ❌ **Real-Time Synchronous Processing**

**Architecture**: API returns results immediately (no queue)

**Why not:**
- **Impossible scaling**: Cannot provision 3,000+ GPUs instantaneously for traffic spikes
- **Cost multiplier**: Need 2-3x more infrastructure for burst capacity headroom
- **Complexity**: Requires sophisticated load balancing and failover
- **User experience**: 100K simultaneous users waiting for AI = poor UX anyway

**Verdict**: Async queue with 60-second SLA is industry standard for AI workloads. Real-time is unnecessary and prohibitively expensive.

---

## Key Architectural Decision

**The core insight**: This is a **burst workload problem**, not a sustained high-traffic problem. 

The recommended approach treats it as such:
- Minimal infrastructure 50 weeks/year
- Elastic scale-up for 2 weeks
- Pay only for what you use
- Queue absorbs traffic spikes gracefully

**Total cost**: ~$1M-1.5M annually vs $50M+ for API approaches or $18M+ for year-round infrastructure.
